<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Zhaoqing Wang - Senior Researcher in Multi-modal Understanding and Generation. Ph.D. Student at Sydney AI Centre and Research Scientist at Pixverse.">
    <meta name="keywords" content="Zhaoqing Wang, Multi-modal, Video Generative Models, Deep Learning, Sydney AI Centre, Pixverse">
    <meta name="author" content="Zhaoqing Wang">
    <title>Zhaoqing Wang (王兆卿) | Senior Researcher</title>
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Styles -->
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <!-- Navigation -->
    <header class="site-header">
        <div class="container">
            <a href="#home" class="logo">Zhaoqing Wang</a>
            <nav class="main-nav">
                <ul>
                    <li><a href="#about">About</a></li>
                    <li><a href="#news">News</a></li>
                    <li><a href="#publications">Publications</a></li>
                    <li><a href="#team">Mentoring</a></li>
                </ul>
                <button id="theme-toggle" aria-label="Toggle Dark Mode">
                    <i class="fas fa-moon"></i>
                </button>
                <button id="mobile-menu-btn" aria-label="Toggle Menu">
                    <i class="fas fa-bars"></i>
                </button>
            </nav>
      </div>
    </header>

    <main>
        <!-- Hero / About Section -->
        <section id="about" class="hero-section">
            <div class="container hero-content">
                <div class="profile-image-container">
                    <img src="images/selfie_v2.png" alt="Zhaoqing Wang" class="profile-image">
    </div>
                <div class="profile-info">
                    <h1>Zhaoqing Wang <span class="chinese-name">(王兆卿)</span></h1>
                    <p class="role">Ph.D. Candidate @ Sydney AI Centre (SAIC)</p>
                    <p class="institution">The University of Sydney</p>
                    
                    <div class="social-links">
                        <a href="mailto:derrickwang005@gmail.com" aria-label="Email"><i class="fas fa-envelope"></i></a>
                        <a href="https://scholar.google.com/citations?hl=en&user=ZqOjPKQAAAAJ" target="_blank" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                        <a href="#" target="_blank" aria-label="GitHub"><i class="fab fa-github"></i></a> <!-- Add GitHub link if available -->
                        <a href="#" target="_blank" aria-label="LinkedIn"><i class="fab fa-linkedin"></i></a>
  </div>

                    <div class="bio-text">
                        <p>
                            I am a third-year Ph.D. student at the <a href="https://www.sydney.edu.au/engineering/our-research/data-science-and-computer-engineering/sydney-artificial-intelligence-centre.html" target="_blank">Sydney AI Centre (SAIC)</a>, University of Sydney, advised by two brilliant supervisors, Prof. <a href="https://tongliang-liu.github.io/" target="_blank">Tongliang Liu</a> and Prof. <a href="https://mingming-gong.github.io/" target="_blank">Mingming Gong</a>.
                        </p>
                        <p style="margin-top: 10px;">
                            Concurrently, I am a Research Scientist at <strong>Pixverse</strong>, where I focus on <strong>large-scale pretraining and supervised finetuning of Video Generative Models</strong>.
                        </p>
                        <p style="margin-top: 10px;">
                          Before that, I was a research intern at Microsoft Research Asia (supervised by <a href="https://scholar.google.com/citations?user=Q7ICmxMAAAAJ&hl=en" target="_blank">Wenlei Shi</a>), OPPO Research Institute (supervised by <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=en" target="_blank">Yandong Guo</a>), Kuaishou Y-Tech (supervised by <a href="https://scholar.google.com/citations?user=GGPvOP4AAAAJ&hl=zh-CN" target="_blank">Qiang Li</a>) and IDL of Baidu Research (supervised by <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=f2Y5nygAAAAJ&sortby=pubdate" target="_blank">Guodong Guo</a>).
                      </p>
                      <p style="margin-top: 10px;">
                            My research currently focuses on <strong>Multi-modal Understanding and Generation</strong>. Previously, I worked on Visual Representation Learning and Visual Perception.
                      </p>
      </div>
    </div>
  </div>
        </section>

        <!-- News Section -->
        <section id="news" class="section">
            <div class="container">
                <h2 class="section-title">News</h2>
                <div class="news-scroll">
                    <ul class="news-list">
                        <!-- Placeholders for recent updates -->
                        <li class="news-item">
                          <span class="date">2025.08</span>
                          <span class="content">One paper (Aligning What Matters) accepted to <strong>NeurIPS 2025</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2025.02</span>
                            <span class="content">One paper (LaVin-DiT) accepted to <strong>CVPR 2025</strong>.</span>
                        </li>
                        <li class="news-item">
                          <span class="date">2024.03</span>
                          <span class="content">I started research scientist at <strong>PixVerse</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2024.01</span>
                            <span class="content">One paper (IDEAL) accepted to <strong>ICLR 2024</strong>.</span>
                        </li>
                        <li class="news-item">
                          <span class="date">2023.05</span>
                          <span class="content">I started internship at <strong>Microsoft Research Asia</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2023.03</span>
                            <span class="content">One paper (BEV-SAN) accepted to <strong>CVPR 2023</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2023.02</span>
                            <span class="content">One paper (MosRep) accepted to <strong>ICLR 2023 (Spotlight)</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2022.08</span>
                            <span class="content">One paper (RSA) accepted to <strong>NeurIPS 2022</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2022.04</span>
                            <span class="content">One paper (PointShift) accepted to <strong>IGARSS 2022 (Oral)</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2022.02</span>
                            <span class="content">Two papers (CRIS, SetSim) accepted to <strong>CVPR 2022</strong>.</span>
                        </li>
                        <li class="news-item">
                          <span class="date">2021.09</span>
                          <span class="content">I started internship at <strong>OPPO Research Institute</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2021.06</span>
                            <span class="content">One paper (CaFM) accepted to <strong>ICCV 2021</strong>.</span>
                        </li>
                        <li class="news-item">
                            <span class="date">2021.03</span>
                            <span class="content">One paper (VecNet) accepted to <strong>IGARSS 2021</strong>.</span>
                        </li>
                        <li class="news-item">
                          <span class="date">2021.02</span>
                          <span class="content">I started internship at <strong>Kuaishou Y-Tech</strong>.</span>
                        </li>
                        <li class="news-item">
                          <span class="date">2020.01</span>
                          <span class="content">I started internship at <strong>IDL of Baidu Research</strong>.</span>
                        </li>
        </ul>
      </div>
    </div>
        </section>

        <!-- Publications Section -->
        <section id="publications" class="section">
            <div class="container">
                <h2 class="section-title">Selected Publications</h2>
                <div class="filter-bar">
                    <button class="filter-btn active" data-filter="all">All</button>
                    <button class="filter-btn" data-filter="cvpr">CVPR</button>
                    <button class="filter-btn" data-filter="iclr">ICLR</button>
  </div>

                <div class="publications-grid">
                    
                    <!-- Aligning What Matters (NeurIPS 2025) -->
                    <article class="paper-card" data-category="selected neurips">
                        <div class="paper-thumb">
                            <img src="images/papers/NeurIPS2025/align.png" alt="Aligning What Matters" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>Aligning What Matters: Masked Latent Adaptation for Text-to-Audio-Video Generation</h3>
                            <p class="authors">Jiyang Zheng, Siqi Pan, Yu Yao, <strong>Zhaoqing Wang</strong>, Dadong Wang, Tongliang Liu</p>
                            <p class="venue"><span class="tag">NeurIPS 2025</span></p>
                            <div class="paper-links">
                                <a href="https://openreview.net/pdf?id=I0hRN2HMeH" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
                            </div>
                        </div>
                    </article>

                    <!-- Lavin-DiT (CVPR 2025) -->
                    <article class="paper-card" data-category="selected cvpr">
                        <div class="paper-thumb">
                            <img src="images/papers/CVPR2025/lavindit.png" alt="Lavin-DiT" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>Lavin-DiT: Large Vision Diffusion Transformer</h3>
                            <p class="authors"><strong>Zhaoqing Wang</strong>, Xiaobo Xia, Runnan Chen, Dongdong Yu, Changhu Wang, Mingming Gong, Tongliang Liu</p>
                            <p class="venue"><span class="tag">CVPR 2025</span></p>
                            <div class="paper-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_LaVin-DiT_Large_Vision_Diffusion_Transformer_CVPR_2025_paper.pdf" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
                            </div>
                        </div>
                    </article>

                    <!-- IDEAL (ICLR 2024) -->
                    <article class="paper-card" data-category="selected iclr">
                        <div class="paper-thumb">
                            <img src="images/papers/ICLR2024/ideal.png" alt="IDEAL" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>IDEAL: Influence-driven Selective Annotations Empower In-context Learners in Large Language Models</h3>
                            <p class="authors">Shaokun Zhang, Xiaobo Xia, <strong>Zhaoqing Wang</strong>, Ling-Hao Chen, Jiale Liu, Qingyun Wu, Tongliang Liu</p>
                            <p class="venue"><span class="tag">ICLR 2024</span></p>
                            <div class="paper-links">
                                <a href="https://arxiv.org/pdf/2310.10873" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
                            </div>
                        </div>
                    </article>

                    <!-- MosRep (ICLR 2023) -->
                    <article class="paper-card" data-category="selected iclr">
                        <div class="paper-thumb">
                            <img src="images/papers/ICLR2023/mosrep.png" alt="MosRep" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>Mosaic Representation Learning for Self-supervised Visual Pre-training</h3>
                            <p class="authors"><strong>Zhaoqing Wang</strong>, Ziyu Chen, Yaqian Li, Yandong Guo, Jun Yu, Mingming Gong, Tongliang Liu</p>
                            <p class="venue"><span class="tag">ICLR 2023</span> <span class="tag highlight">Spotlight</span></p>
                            <div class="paper-links">
                                <a href="https://openreview.net/pdf?id=JAezPMehaUu" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
                                <!-- <a href="#" class="btn-link"><i class="fab fa-github"></i> Code</a> -->
                            </div>
                        </div>
                    </article>

                    <!-- BEV-SAN (CVPR 2023) -->
                    <article class="paper-card" data-category="selected cvpr">
                        <div class="paper-thumb">
                            <img src="images/papers/CVPR2023/bev-san.png" alt="BEV-SAN" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks</h3>
                            <p class="authors">Xiaowei Chi, Jiaming Liu, Ming Lu, Rongyu Zhang, <strong>Zhaoqing Wang</strong>, Yandong Guo, Shanghang Zhang</p>
                            <p class="venue"><span class="tag">CVPR 2023</span></p>
                            <div class="paper-links">
                                <a href="https://openreview.net/pdf?id=jtBCXHQYYX" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
        </div>
      </div>
                    </article>

                    <!-- RSA (NeurIPS 2022) -->
                    <article class="paper-card" data-category="selected neurips">
                        <div class="paper-thumb">
                            <img src="images/papers/NeurIPS2022/rsa.png" alt="RSA" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>RSA: Reducing Semantic Shift from Aggressive Augmentations for Self-supervised Learning</h3>
                            <p class="authors">Yingbin Bai, Erkun Yang, <strong>Zhaoqing Wang</strong>, Yuxuan Du, Bo Han, Cheng Deng, Dadong Wang, Tongliang Liu</p>
                            <p class="venue"><span class="tag">NeurIPS 2022</span></p>
                            <div class="paper-links">
                                <a href="https://openreview.net/pdf?id=Cgmk9CicWFl" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
        </div>
      </div>
                    </article>

                    <!-- CRIS (CVPR 2022) -->
                    <article class="paper-card" data-category="selected cvpr">
                        <div class="paper-thumb">
                            <img src="images/papers/CVPR2022/cris.png" alt="CRIS" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>CRIS: CLIP-Driven Referring Image Segmentation</h3>
                            <p class="authors"><strong>Zhaoqing Wang</strong>, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, Tongliang Liu</p>
                            <p class="venue"><span class="tag">CVPR 2022</span></p>
                            <div class="paper-links">
                                <a href="https://arxiv.org/pdf/2111.15174" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
        </div>
      </div>
                    </article>

                    <!-- SetSim (CVPR 2022) -->
                    <article class="paper-card" data-category="selected cvpr">
                        <div class="paper-thumb">
                            <img src="images/papers/CVPR2022/setsim.png" alt="SetSim" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>Exploring Set Similarity for Dense Self-supervised Representation Learning</h3>
                            <p class="authors"><strong>Zhaoqing Wang</strong>, Qiang Li, Guoxin Zhang, Pengfei Wan, Wen Zheng, Nannan Wang, Mingming Gong, Tongliang Liu</p>
                            <p class="venue"><span class="tag">CVPR 2022</span></p>
                            <div class="paper-links">
                                <a href="https://arxiv.org/pdf/2107.08712" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
        </div>
      </div>
                    </article>

                    <!-- CaFM (ICCV 2021) -->
                    <article class="paper-card" data-category="iccv">
                        <div class="paper-thumb">
                            <img src="images/papers/ICCV2021/cafm.png" alt="CaFM" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation</h3>
                            <p class="authors">Jiaming Liu, Ming Lu, Kaixin Chen, Xiaoqi Li, Shizun Wang, <strong>Zhaoqing Wang</strong>, Enhua Wu, Yurong Chen, Chuang Zhang, Ming Wu</p>
                            <p class="venue"><span class="tag">ICCV 2021</span></p>
                            <div class="paper-links">
                                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Overfitting_the_Data_Compact_Neural_Video_Delivery_via_Content-Aware_Feature_ICCV_2021_paper.pdf" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
        </div>
      </div>
                    </article>

                    <!-- PointShift (IGARSS 2022) -->
                    <article class="paper-card" data-category="igarss">
                        <div class="paper-thumb">
                            <img src="images/papers/IGARSS2022/pointshift.png" alt="PointShift" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>PointShift: Point-wise Shift MLP for Pixel-level Cloud Type Classification</h3>
                            <p class="authors">Yixiang Huang, <strong>Zhaoqing Wang</strong>, Xin Jiang, Ming Wu, Ming Wu, Chuang Zhang, Chuang Zhang, Jun Guo</p>
                            <p class="venue"><span class="tag">IGARSS 2022</span> <span class="tag highlight">Oral</span></p>
                            <div class="paper-links">
                                <a href="https://ieeexplore.ieee.org/abstract/document/9554737" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
        </div>
      </div>
                    </article>

                    <!-- VecNet (IGARSS 2021) -->
                    <article class="paper-card" data-category="igarss">
                        <div class="paper-thumb">
                            <img src="images/papers/IGARSS2021/vecnet.png" alt="VecNet" loading="lazy">
                        </div>
                        <div class="paper-info">
                            <h3>Vecnet: A Spectral and Multi-Scale Spatial Fusion Deep Network for Pixel-Level Cloud Type Classification</h3>
                            <p class="authors"><strong>Zhaoqing Wang</strong>, Xiangyu Kong, Zhanbei Cui, Ming Wu, Chuang Zhang, MingMing Gong, Tongliang Liu</p>
                            <p class="venue"><span class="tag">IGARSS 2021</span></p>
                            <div class="paper-links">
                                <a href="https://ieeexplore.ieee.org/abstract/document/9554737" target="_blank" class="btn-link"><i class="far fa-file-pdf"></i> PDF</a>
                            </div>
        </div>
                    </article>

      </div>

                <div class="view-all-container">
                    <a href="https://scholar.google.com/citations?hl=en&user=ZqOjPKQAAAAJ" target="_blank" class="btn primary">View Full List on Google Scholar</a>
        </div>
      </div>
        </section>

        <!-- Mentoring / Team Section (Placeholder) -->
        <section id="team" class="section">
            <div class="container">
                <h2 class="section-title">Mentoring & Professional Activities</h2>
                <div class="services-content">
                    <div class="service-block">
                        <h3>Reviewer Service</h3>
                        <p>CVPR, ICCV, ECCV, ICLR, NeurIPS, ICML, AAAI, IGARSS, ICPR, ACM Computing Surveys, T-PAMI, PR</p>
          </div>
        </div>
      </div>
        </section>

    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2026 Zhaoqing Wang. Built with modern web technologies.</p>
      </div>
    </footer>

    <script src="js/main.js"></script>
</body>
</html>